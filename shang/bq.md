the interactive warehouse project where I lead a team to reduced the Presto query latency for unidash and daiquery and save a lot engineering time. Start with some basic intro about Presto's architecture, like coordinator, worker, query split up to stage and then further to tasks, then coordinator sending task to worker for execution on data. Then Focus on what I did including divide the traffice to 3 categories: high priority, interactive, regular where the 1st are severing importing dashboards with a data input size of 100GB, the 2nd are for queries where there are users waiting for result, and the third one are for queries where people fire and forget. I used the client tag that come with the query to identify wheter it is unidash (mainly for dashboard) and daiquery (interactive and adhoc queries). In the meantime, I worked with another team which is the one that sends queries to us, to set up 1, cache for repeated queries, 2, data size estimation after query return and use that size for futher queries so that less than 100GB can be prioritied. This separetion remove the noisy neighbor issue. I also delegate the task to a colleage to add memcache to store metadata so we dont have to fetch from remote service, while myself dig into the presto architecture and code to identify a few bottlenecks on the most critical path, task scheduling, a.k.a presto coordiantor send data partition info to workers so that worker can fetch the data from storage and work on it from runtime metrics collection and profiling (some context info, presto worker has been rewritten in cpp from java and become super fast so that worker execution is not the bottleneck): one, task scheduling, which is very critical given presto's architecture, is slow due to contention especailly when big queries that can have dozens of stages and hundreds of tasks, so I re-architecture the multiple threading model for thread pool, a fire and forget pattern to event loop so that all method calls for the same task can be serialized in a queue, this remove the necessarity of locks but still allow the parallles of differnt tasks from the same query. two,  serialization and deserailzation is low due to the json. So I migrated it to thrift. the challenge is that there are so many existing java classes with its logic in the code. if we take approach of writing the idl file manually, and then generate .java class file for each class, we will need another help class to hold all the logic for each of them. Basically, for all 200+ classes, we need double .java file. I come up with this idea of using java annotation to dynamically generate .java class file necessary for thrift. Given that presto worker, a.k.a the server side for the communication is written in cpp,the annotation based solution also allow us to auto gerneate the idl file and genreate related .cpp file for the worker side. This solution use the java code and its annoation as source of truth, keep the design and code clean and easity to maintain. three, presto was still using http1.1 for communication between coordinator and worker which does not support multiplex. Another team member help to upgrade it to http2 after we went through af ew java library options and finally agree on reactive-netty  where multipling is supported, no head of line blocking, both header and payload can be compressed with meta zstd compression. This combined with thrift serde reduce the network payload and transimittion time so that worker can get tasks faster.
